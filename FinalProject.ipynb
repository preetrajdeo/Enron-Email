{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In 2000, Enron was one of the largest companies in the United States. By 2002, it had collapsed into bankruptcy due to widespread corporate fraud. In the resulting Federal investigation, there was a significant amount of typically confidential information entered into public record, including tens of thousands of emails and detailed financial data for top executives. In this project, you will play detective, and put your new skills to use by building a person of interest identifier based on financial and email data made public as a result of the Enron scandal. To assist you in your detective work, we've combined this data with a hand-generated list of persons of interest in the fraud case, which means individuals who were indicted, reached a settlement, or plea deal with the government, or testified in exchange for prosecution immunity.\n",
    "\n",
    "In this project I will be engineering the features, pick and tune an algorithm, test and evaluate my identifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Task 1: is to load the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "\n",
    "from feature_format import featureFormat\n",
    "from feature_format import targetFeatureSplit\n",
    "\n",
    "### features_list is a list of strings, each of which is a feature name\n",
    "### first feature must be \"poi\", as this will be singled out as the label\n",
    "features_list = [\"poi\"]\n",
    "\n",
    "### load the dictionary containing the dataset\n",
    "data_dict = pickle.load(open(\"final_project_dataset.pkl\", \"r\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Task 2: Remove outliers. I have removed the outlier \"Total\" and  \"The Travel Agency In The Park\" as they didn't have any relevance in the dataset. I also go ahead and remove all Nan's from the 'Salary' column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "features = [\"salary\", \"bonus\"]\n",
    "data = featureFormat(data_dict, features)\n",
    "for point in data:\n",
    "    salary = point[0]\n",
    "    bonus = point[1]\n",
    "    plt.scatter(salary, bonus)\n",
    "\n",
    "plt.xlabel(\"salary\")\n",
    "plt.ylabel(\"bonus\")\n",
    "#plt.show()\n",
    "outliers = [\"TOTAL\", 'The TRAVE AGENCY IN THE PARK']\n",
    "\n",
    "def remove_outliers():\n",
    "    outliers = ['TOTAL', 'THE TRAVEL AGENCY IN THE PARK']\n",
    "    for x in outliers:\n",
    "        data_dict.pop(x, 0)\n",
    "\n",
    "    return data_dict\n",
    "\n",
    "### remove NAN's from dataset\n",
    "outliers = []\n",
    "for key in data_dict:\n",
    "    val = data_dict[key]['salary']\n",
    "    if val == 'NaN':\n",
    "        continue\n",
    "    outliers.append((key, int(val)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "######  Task 3: Create new feature(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### create new features\n",
    "### new features are: fraction_to_poi_email,fraction_from_poi_email\n",
    "\n",
    "def dict_to_list(var1,var2):\n",
    "    new_list=[]\n",
    "\n",
    "    for i in data_dict:\n",
    "        if data_dict[i][var1]==\"NaN\" or data_dict[i][var2]==\"NaN\":\n",
    "            new_list.append(0.)\n",
    "        elif data_dict[i][var1]>=0:\n",
    "            new_list.append(float(data_dict[i][var1])/float(data_dict[i][var2]))\n",
    "    return new_list\n",
    "\n",
    "### create two lists of new features\n",
    "fraction_from_poi_email=dict_to_list(\"from_poi_to_this_person\",\"to_messages\")\n",
    "fraction_to_poi_email=dict_to_list(\"from_this_person_to_poi\",\"from_messages\")\n",
    "\n",
    "### Now its time to insert those features in data_dict\n",
    "counter = 0\n",
    "for i in data_dict:\n",
    "    data_dict[i][\"fraction_from_poi_email\"] = fraction_from_poi_email[counter]\n",
    "    data_dict[i][\"fraction_to_poi_email\"]=fraction_to_poi_email[counter]\n",
    "    counter +=1\n",
    "\n",
    "#Now I add my two new features to my list of variables. \n",
    "    \n",
    "features_list = [\"poi\", \"fraction_from_poi_email\", \"fraction_to_poi_email\"] \n",
    "\n",
    "### store to my_dataset for easy export below\n",
    "my_dataset = data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "However the dataset comes with a number of variables. \n",
    "Let's use a decision tree to find out which variable is important and which\n",
    "is not. We will use accuracy, precision and recall to make our decision. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.866666666667\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "      <th>Importances</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>salary</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bonus</td>\n",
       "      <td>0.077381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fraction_from_poi_email</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fraction_to_poi_email</td>\n",
       "      <td>0.232919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>deferral_payments</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>total_payments</td>\n",
       "      <td>0.058036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>loan_advances</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>restricted_stock_deferred</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>deferred_income</td>\n",
       "      <td>0.140117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>total_stock_value</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>expenses</td>\n",
       "      <td>0.010031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>exercised_stock_options</td>\n",
       "      <td>0.118986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>long_term_incentive</td>\n",
       "      <td>0.053737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>shared_receipt_with_poi</td>\n",
       "      <td>0.255057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>restricted_stock</td>\n",
       "      <td>0.053737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>director_fees</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Features  Importances\n",
       "0                      salary     0.000000\n",
       "1                       bonus     0.077381\n",
       "2     fraction_from_poi_email     0.000000\n",
       "3       fraction_to_poi_email     0.232919\n",
       "4           deferral_payments     0.000000\n",
       "5              total_payments     0.058036\n",
       "6               loan_advances     0.000000\n",
       "7   restricted_stock_deferred     0.000000\n",
       "8             deferred_income     0.140117\n",
       "9           total_stock_value     0.000000\n",
       "10                   expenses     0.010031\n",
       "11    exercised_stock_options     0.118986\n",
       "12        long_term_incentive     0.053737\n",
       "13    shared_receipt_with_poi     0.255057\n",
       "14           restricted_stock     0.053737\n",
       "15              director_fees     0.000000"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Let's make a separate dataset to use in this decision tree so that we can use my_dataset unchanged \n",
    "##later on. \n",
    "features_list = [\"poi\", \"salary\", \"bonus\", \"fraction_from_poi_email\", \"fraction_to_poi_email\",\n",
    "                 'deferral_payments', 'total_payments', 'loan_advances', 'restricted_stock_deferred',\n",
    "                 'deferred_income', 'total_stock_value', 'expenses', 'exercised_stock_options',\n",
    "                 'long_term_incentive', 'shared_receipt_with_poi', 'restricted_stock', 'director_fees']\n",
    "\n",
    "data_dt = featureFormat(my_dataset, features_list)\n",
    "\n",
    "#Now its time to split the dataset into features and labels. The code below assumes that the \n",
    "#first variable is the label.\n",
    "\n",
    "labels, features = targetFeatureSplit(data_dt)\n",
    "\n",
    "#Let's split the dataset into training and testing data\n",
    "from sklearn import cross_validation\n",
    "features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(\n",
    "    features,labels,test_size=0.1, random_state=42)\n",
    "\n",
    "#Now let's deploy the decision tree and calculate the accuracy of it by using the testing set\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(features_train,labels_train)\n",
    "score = clf.score(features_test,labels_test)\n",
    "pred= clf.predict(features_test)\n",
    "print 'Accuracy:', score\n",
    "print 'Precision:', precision_score(labels_test, pred)  \n",
    "print 'Recall:', recall_score(labels_test, pred)\n",
    "\n",
    "importances = clf.feature_importances_\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data_imp = pd.DataFrame(\n",
    "    {'Features': features_list[1:], \n",
    "     'Importances': importances})\n",
    "data_imp\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The precision and recall is 0 in this decision tree. Hence, I manually try using the decision tree with a combination of features. I come up with these three that give me a precision and recall of 0.67:\n",
    "* fraction_to_poi_email\n",
    "* expenses\n",
    "* shared_receipt_with_poi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy, Precision and Recall before tuning the algorithm:\n",
      "Accuracy: 0.692307692308\n",
      "Precision: 0.4\n",
      "Recall: 0.666666666667\n",
      "Accuracy, Precision and Recall after tuning the algorithm:\n",
      "Accuracy: 0.846153846154\n",
      "Precision: 0.666666666667\n",
      "Recall: 0.666666666667\n"
     ]
    }
   ],
   "source": [
    "##According to these results I pick the following features. \n",
    "features_list_rev = ['poi', 'fraction_to_poi_email',\n",
    "                     'expenses', 'shared_receipt_with_poi']\n",
    "##Now I deply the decision tree again for the new features. \n",
    "data_dt_rev = featureFormat(data_dict, features_list_rev)\n",
    "\n",
    "#Now its time to split the dataset into features and labels. The code below assumes that the first variable is the \n",
    "#label.\n",
    "\n",
    "labels, features = targetFeatureSplit(data_dt_rev)\n",
    "\n",
    "#Let's split the dataset into training and testing data\n",
    "from sklearn import cross_validation\n",
    "features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(\n",
    "    features, labels, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "#Now let's deploy the decision tree and calculate the accuracy, precision and recall before \n",
    "#tuning the algorithm\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(features_train,labels_train)\n",
    "score = clf.score(features_test,labels_test)\n",
    "pred= clf.predict(features_test)\n",
    "print 'Accuracy, Precision and Recall before tuning the algorithm:'\n",
    "print 'Accuracy:', score\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "print 'Precision:', precision_score(labels_test, pred)  \n",
    "print 'Recall:', recall_score(labels_test, pred)\n",
    "\n",
    "\n",
    "#Now let's deploy the decision tree and calculate the accuracy, precision and recall before \n",
    "#tuning the algorithm after tuning the algorithm. A min_samples_split = 6 or higher gives the best\n",
    "#precision and recall.\n",
    "\n",
    "\n",
    "clf = DecisionTreeClassifier(min_samples_split = 11)\n",
    "clf.fit(features_train,labels_train)\n",
    "score = clf.score(features_test,labels_test)\n",
    "pred= clf.predict(features_test)\n",
    "print 'Accuracy, Precision and Recall after tuning the algorithm:'\n",
    "print 'Accuracy:', score\n",
    "print 'Precision:', precision_score(labels_test, pred)  \n",
    "print 'Recall:', recall_score(labels_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Dump your classifier, dataset ad features list so that anyone can run it and check your result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(clf, open(\"my_classifier.pkl\", \"w\") )\n",
    "pickle.dump(data_dict, open(\"my_dataset.pkl\", \"w\") )\n",
    "pickle.dump(features_list, open(\"my_feature_list.pkl\", \"w\") )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "In this project, since we are trying to identify the Person of Interest(POI), we can say that according to the decision tree algorithm, we can say that 67% of the time that our algorithm identified a POI it really was a POI because our precision is 0.67 and 33% of the time if it flagged a person as POI, then it was a false alarm. Recall tells us that our algorithm s 67% of the time correct in flagging the POI while the other 33% of the time it was showing us a false negative which means that it would not recognize a person as a POI while he/she actually was. While our numbers seem to be reasonable, there is obviously room for improvement. We can keep look into the text of the data and see if we can garner any more information from there. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
